<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data Engineer on D i o g o D a t a</title><link>https://diogo-dantas.github.io/categories/data-engineer/</link><description>Recent content in Data Engineer on D i o g o D a t a</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Fri, 20 Sep 2024 06:00:23 +0600</lastBuildDate><atom:link href="https://diogo-dantas.github.io/categories/data-engineer/index.xml" rel="self" type="application/rss+xml"/><item><title>ETL simplified: unified data</title><link>https://diogo-dantas.github.io/posts/engenharia-de-dados/python/etl-simplificado/</link><pubDate>Fri, 20 Sep 2024 06:00:23 +0600</pubDate><guid>https://diogo-dantas.github.io/posts/engenharia-de-dados/python/etl-simplificado/</guid><description>&lt;p>In today&amp;rsquo;s dynamic business environment, integrating and analysing data from multiple sources is essential for making informed decisions. In many industries, such as finance, logistics or marketing, it is common to receive data files in a variety of formats, including CSV, XML and JSON. These files can contain valuable information, but their dispersion and variety of formats make processing and analysis a significant challenge.&lt;/p>
&lt;p>One of the challenges in this scenario is the need to consolidate information from multiple files into a single format that can be easily imported into a database, for example. The task becomes even more critical when considering the need to maintain data integrity and consistency during this compilation process.&lt;/p></description></item></channel></rss>